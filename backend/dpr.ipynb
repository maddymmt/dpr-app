{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\dpr-app\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import PreProcessor, PDFToTextConverter\n",
    "from haystack.nodes.retriever.dense import DensePassageRetriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.nodes import FARMReader\n",
    "from haystack.utils import convert_files_to_docs\n",
    "from tqdm.auto import tqdm\n",
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the document store\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert files to dicts containing the text of the documents\n",
    "# dicts = convert_files_to_docs(dir_path='F:\\Projects\\dpr-app\\backend\\deephashing.pdf')\n",
    "# print(len(dicts))\n",
    "\n",
    "pdf_converter = PDFToTextConverter(remove_numeric_tables=True, valid_languages=[\"en\"])\n",
    "doc_pdf = pdf_converter.convert(file_path='docs/deephashing.pdf', meta=None)\n",
    "# print(doc_pdf)  # This should show the content of the PDF if conversion is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/1 [00:00<?, ?it/s]We found one or more sentences whose word count is higher than the split length.\n",
      "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 18.18docs/s]\n",
      "Processing documents: 100%|██████████| 1/1 [00:00<00:00, 15.15it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PreProcessor(split_length=100, split_overlap=0, split_respect_sentence_boundary=True)\n",
    "processed_docs = []\n",
    "for doc in tqdm(doc_pdf, desc=\"Processing documents\"):\n",
    "    processed = preprocessor.process([doc])  # process each document individually\n",
    "    processed_docs.extend(processed)  # extend the list with the results\n",
    "document_store.write_documents(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\dpr-app\\.venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DPR Retriever with GPU enabled\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents Processed: 10000 docs [00:40, 247.43 docs/s]         \n"
     ]
    }
   ],
   "source": [
    "# Update the embeddings for our documents in the document store\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a reader with GPU enabled\n",
    "model_name_or_path = \"deepset/roberta-base-squad2\"\n",
    "reader = FARMReader(model_name_or_path, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Extractive QA Pipeline\n",
    "pipe = ExtractiveQAPipeline(reader=reader, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a query\n",
    "queries = [\"What is deep hashing?\", \"How does LSH work?\", \"What are hash functions?\"]\n",
    "\n",
    "# results = pipe.run(query=\"What is deep hashing?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.88s/ Batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep hashing?\n",
      "'Query: What is deep hashing?'\n",
      "'Answers:'\n",
      "[   <Answer {'answer': 'a single cosine similarity based learning objective', 'type': 'extractive', 'score': 0.6755504012107849, 'context': 'nd Tao Xiang. One\\nloss for all: Deep hashing with a single cosine similarity based learning objective. Advances in Neural\\nInformation Processing Syste', 'offsets_in_document': [{'start': 319, 'end': 370}], 'offsets_in_context': [{'start': 50, 'end': 101}], 'document_ids': ['5523ea9adaf6d87c7b8cceac2e6b2e39'], 'meta': {'_split_id': 74}}>,\n",
      "    <Answer {'answer': 'state-of-the-art methods', 'type': 'extractive', 'score': 0.6514706611633301, 'context': 'e significant advances brought about by deep\\nlearning in image tasks, deep hashing methods [45,50] have become state-of-the-art methods in the field.\\n', 'offsets_in_document': [{'start': 604, 'end': 628}], 'offsets_in_context': [{'start': 111, 'end': 135}], 'document_ids': ['cd866affb03996661a329c236903c0d5'], 'meta': {'_split_id': 5}}>,\n",
      "    <Answer {'answer': 'Householder Quantization\\nLucas R. Schwengber†1, Lucas Resende†1, Paulo Orenstein1, and Roberto I. Oliveira1\\n1IMPA, Rio de Janeiro, Brazil\\nAbstract\\nHashing is at the heart of large-scale image similarity search', 'type': 'extractive', 'score': 0.5409374833106995, 'context': ' Householder Quantization\\nLucas R. Schwengber†1, Lucas Resende†1, Paulo Orenstein1, and Roberto I. Oliveira1\\n1IMPA, Rio de Janeiro, Brazil\\nAbstract\\nHashing is at the heart of large-scale image similarity search', 'offsets_in_document': [{'start': 17, 'end': 226}], 'offsets_in_context': [{'start': 1, 'end': 210}], 'document_ids': ['681ccc535865bc77d0e161e575c450a1'], 'meta': {'_split_id': 0}}>,\n",
      "    <Answer {'answer': 'look for solutions', 'type': 'extractive', 'score': 0.5034641027450562, 'context': 'That is, deep\\nhashing methods look for solutions such that (i) the learned embeddings preserve similarity well, which is\\nsolved through a similarity l', 'offsets_in_document': [{'start': 30, 'end': 48}], 'offsets_in_context': [{'start': 30, 'end': 48}], 'document_ids': ['d0f0e6e661561d6e21d9b11bcd2d4afc'], 'meta': {'_split_id': 7}}>,\n",
      "    <Answer {'answer': 'Deep Hashing Network', 'type': 'extractive', 'score': 0.44951826333999634, 'context': 'On the other hand, Deep Hashing Network (DHN) [63]\\nconsiders a pairwise cross-entropy loss for the similarity term, while using the same L1\\nquantizati', 'offsets_in_document': [{'start': 19, 'end': 39}], 'offsets_in_context': [{'start': 19, 'end': 39}], 'document_ids': ['404e1020975729ca42bb5ed64d52bdba'], 'meta': {'_split_id': 16}}>]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.77s/ Batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does LSH work?\n",
      "'Query: How does LSH work?'\n",
      "'Answers:'\n",
      "[   <Answer {'answer': 'assigns\\ncompact binary hash codes to images', 'type': 'extractive', 'score': 0.2829137146472931, 'context': 'r example, Locality-Sensitive Hashing (LSH) [2,17,21] assigns\\ncompact binary hash codes to images such that similar items receive similar hash codes, ', 'offsets_in_document': [{'start': 192, 'end': 235}], 'offsets_in_context': [{'start': 54, 'end': 97}], 'document_ids': ['72a06bf473f74c0387c0990cd6307679'], 'meta': {'_split_id': 3}}>,\n",
      "    <Answer {'answer': 'By using pre-trained architectures', 'type': 'extractive', 'score': 0.05957616865634918, 'context': 'th a sequence of fully connected layers to\\nbe fine-tuned. By using pre-trained architectures, they exploit the enriched features and start the trainin', 'offsets_in_document': [{'start': 208, 'end': 242}], 'offsets_in_context': [{'start': 58, 'end': 92}], 'document_ids': ['87343af840ed03af1590b98905b5e3cb'], 'meta': {'_split_id': 14}}>,\n",
      "    <Answer {'answer': 'i,j\\nsij\\n⟨oi\\n, oj\\n⟩ + log\\n1 + e−⟨oi,oj⟩\\nand\\nLQ\\n=\\nn\\nn\\nk\\nlog\\ncosh |oil\\n|\\ne', 'type': 'extractive', 'score': 0.035997651517391205, 'context': ' Network (DHN) [63]\\nFor this loss,\\nLS\\n=\\ni,j\\nsij\\n⟨oi\\n, oj\\n⟩ + log\\n1 + e−⟨oi,oj⟩\\nand\\nLQ\\n=\\nn\\nn\\nk\\nlog\\ncosh |oil\\n|\\ne\\n.\\nWe let a batch size of 64, as recomm', 'offsets_in_document': [{'start': 58, 'end': 129}], 'offsets_in_context': [{'start': 40, 'end': 111}], 'document_ids': ['206f6341cbbed400bd27efbc61858115'], 'meta': {'_split_id': 96}}>,\n",
      "    <Answer {'answer': '−\\nn\\nC\\nyil', 'type': 'extractive', 'score': 0.02972247265279293, 'context': 'ding” of a class. Let p1\\n, . . . , pC\\n∈ Rk be the proxies. Define\\nLP\\n= −\\nn\\nC\\nyil\\n⟨oi,pl⟩\\n∥oi∥∥pl∥\\nn\\nC\\nyil\\n+\\nn\\nC\\n(1 − yil\\n) ⟨oi,pl⟩\\n∥oi∥∥pl∥\\n− δ\\n+\\nn\\nC\\n', 'offsets_in_document': [{'start': 114, 'end': 123}], 'offsets_in_context': [{'start': 71, 'end': 80}], 'document_ids': ['45d40815ff2512eac1610e5db4b91b96'], 'meta': {'_split_id': 99}}>,\n",
      "    <Answer {'answer': 'sij\\np\\n+\\n1 − sij\\n1 − p', 'type': 'extractive', 'score': 0.022503990679979324, 'context': 'een the amount of\\nsimilar and dissimilar pairs. We define:\\nwij\\n=\\nsij\\np\\n+\\n1 − sij\\n1 − p\\n.\\nwhere p is the percentage of similar pairs in the training se', 'offsets_in_document': [{'start': 368, 'end': 389}], 'offsets_in_context': [{'start': 65, 'end': 86}], 'document_ids': ['126d9fc62b0ac133d121b98fd0edf978'], 'meta': {'_split_id': 92}}>]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:03<00:00,  3.58s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are hash functions?\n",
      "'Query: What are hash functions?'\n",
      "'Answers:'\n",
      "[   <Answer {'answer': 'two-termed loss function', 'type': 'extractive', 'score': 0.15098091959953308, 'context': 'he similarity learning and\\nquantization strategies in a single two-termed loss function where one term accounts for similarity learning\\nand the other ', 'offsets_in_document': [{'start': 380, 'end': 404}], 'offsets_in_context': [{'start': 63, 'end': 87}], 'document_ids': ['d0f0e6e661561d6e21d9b11bcd2d4afc'], 'meta': {'_split_id': 7}}>,\n",
      "    <Answer {'answer': 'discrete nature', 'type': 'extractive', 'score': 0.12224160134792328, 'context': 'een proposed (e.g., , [6,7,37,45,51,53,58,63]).\\nHowever, due to the discrete nature of hash functions, the objective function in (3.1) is not differen', 'offsets_in_document': [{'start': 370, 'end': 385}], 'offsets_in_context': [{'start': 68, 'end': 83}], 'document_ids': ['b733a44dadcf150e6f21f6e3e45ca9be'], 'meta': {'_split_id': 25}}>,\n",
      "    <Answer {'answer': 'bi\\n, bj\\n∈ {−1, 1}k are hash codes', 'type': 'extractive', 'score': 0.07732778787612915, 'context': ') =\\nk − ⟨bi\\n, bj\\n⟩\\n=\\nk\\n⟨bi\\n, bj\\n⟩\\n∥bi\\n∥bj\\nwhich holds when bi\\n, bj\\n∈ {−1, 1}k are hash codes. This identity relates the Hamming distance with the\\ninne', 'offsets_in_document': [{'start': 130, 'end': 163}], 'offsets_in_context': [{'start': 59, 'end': 92}], 'document_ids': ['5b5e769de32ab180f5ba9c5f35c2a2a2'], 'meta': {'_split_id': 26}}>,\n",
      "    <Answer {'answer': 'nearest neighbor\\nsearch while retaining good retrieval quality', 'type': 'extractive', 'score': 0.05648736655712128, 'context': 'een employed to quickly perform approximate nearest neighbor\\nsearch while retaining good retrieval quality. For example, Locality-Sensitive Hashing (L', 'offsets_in_document': [{'start': 72, 'end': 134}], 'offsets_in_context': [{'start': 44, 'end': 106}], 'document_ids': ['72a06bf473f74c0387c0990cd6307679'], 'meta': {'_split_id': 3}}>,\n",
      "    <Answer {'answer': '∥ tanh(βoi\\n) − hi\\nsmall', 'type': 'extractive', 'score': 0.03716578334569931, 'context': 'te\\ntheir loss in tanh(βoi\\n), thus making the quantization error\\n∥ tanh(βoi\\n) − hi\\nsmall. This quantization strategy is quite interesting, but is out o', 'offsets_in_document': [{'start': 167, 'end': 190}], 'offsets_in_context': [{'start': 64, 'end': 87}], 'document_ids': ['8be3ca1f439ab93d4e303fb15e19fbcc'], 'meta': {'_split_id': 101}}>]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    results = pipe.run(query=query, params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})\n",
    "    print(f\"Query: {query}\")\n",
    "    print_answers(results, details=\"all\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Output the results\n",
    "# if results['answers']:\n",
    "#     for idx, answer_obj in enumerate(results['answers']):\n",
    "#         answer = answer_obj.__dict__  # Convert the Answer object to a dictionary if necessary\n",
    "#         print(f\"\\nAnswer {idx+1}:\")\n",
    "#         print(f\"  Text: {answer['answer']}\")\n",
    "#         print(f\"  Score: {answer['score']:.4f}\")\n",
    "#         context = answer['context'] if 'context' in answer else \"Not provided\"\n",
    "#         print(f\"  Context: {context}\")\n",
    "#         doc_id = answer['document_ids'][0] if 'document_ids' in answer else \"Not provided\"\n",
    "#         print(f\"  Document ID: {doc_id}\")\n",
    "#         # Handle Span object or dictionary for offsets\n",
    "#         start_pos = answer['offsets_in_document'][0].start if hasattr(answer['offsets_in_document'][0], 'start') else answer['offsets_in_document'][0]['start']\n",
    "#         end_pos = answer['offsets_in_document'][0].end if hasattr(answer['offsets_in_document'][0], 'end') else answer['offsets_in_document'][0]['end']\n",
    "#         print(f\"  Start: {start_pos}, End: {end_pos}\")\n",
    "# else:\n",
    "#     print(\"\\nNo answers found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_answers(results, details=\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
